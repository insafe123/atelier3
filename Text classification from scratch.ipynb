{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eafb4d3",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b01fb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d89a1",
   "metadata": {},
   "source": [
    "# generate the training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae826b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n",
      "Using 60000 files for training.\n",
      "Found 75000 files belonging to 3 classes.\n",
      "Using 15000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 1875\n",
      "Number of batches in raw_val_ds: 469\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c032a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'SPOILERS: We sit through ten minutes of AWFUL clich\\xc3\\xa9d dialog at the beginning from two completely unoriginal characters with bad twangs (ripped off from Kalifornia and Natural Born Killers - there isn\\'t an original thing about these two) and you\\'re going \"either they\\'re about to kill everyone in the diner or already have\" and lo and behold guess what happens.<br /><br />I can\\'t stand all the Tarantino wannabes out there and this guy is one of the worst. I got maybe 25-30 minutes into the thing when I just couldn\\'t take it and stopped watching. Miner\\'s really bad acting was unbearable - I couldn\\'t take it. That, and the terrible script. After reading some of these comments I see there was a big twist - well guess what? No one cares. When you create completely uninteresting, unoriginal and unlikeable character like these two clich\\xc3\\xa9s, no one cares what big \"twist\" may have happened. I hope this is the end of these types of movies.'\n",
      "2\n",
      "b'This movie is horrible- in a \\'so bad it\\'s good\\' kind of way.<br /><br />The storyline is rehashed from so many other films of this kind, that I\\'m not going to even bother describing it. It\\'s a sword/sorcery picture, has a kid hoping to realize how important he is in this world, has a \"nomadic\" adventurer, an evil aide/sorcerer, a princess, a hairy creature....you get the point.<br /><br />The first time I caught this movie was during a very harsh winter. I don\\'t know why I decided to continue watching it for an extra five minutes before turning the channel, but when I caught site of Gulfax, I decided to stay and watch it until the end.<br /><br />Gulfax is a white, furry creature akin to Chewbacca, but not nearly as useful or entertaining to watch. He looks like someone glued a bunch of white shag carpeting together and forced the actor to wear it. There are scenes where it looks like the actor cannot move within, or that he\\'s almost falling over. Although he isn\\'t in the movie that much, the few scenes are worth it! Watch as he attempts to talk smack to Bo Svenson, taking the Solo-Chewbacca comparison\\'s to an even higher level! <br /><br />I actually bought this movie just because of that character, and still have it somewhere! <br /><br />Gulfax may look like sh!t, but he made this movie!!! The only reason I\\'ve never seen the sequel, or even sought it out, was because of his absence! Perhaps should there be a final film, completing the trilogy, Gulfax will make a much-anticipated return!'\n",
      "1\n",
      "b\"Pierce Brosnan has sipped his last Martini and returns, in an outrageous self-parody, as the aging foul-mouthed boozy assassin Julian Noble, who has a particular fondness for teenage girls, bullfights and tacky clothes. During a job in Mexico City he meets Danny (Greg Kinnear), a straight-faced Denver suburban business-man, who's in town to make his deal of-a-life-time, in a hotel bar. Despite their completely different personalities and Julian's crude and insensible remarks, they become friends. <br /><br />Largely carried by the performances of Pierce Brosnan and Greg Kinnear, director Richard Shepard revealed that he didn't write the film with Pierce Brosnan in mind , but I can hardly imagine this without him. He proves to have a real talent for comedy and can be more than just James Bond or cold-war spies. The scene in which the two meet at a glossy hotel bar (stunning sets and beautifully photographed) really is a bravura piece of acting skills. The scene lasts almost fifteen minutes, and although it was probably carefully scripted, the two actors are largely improvising, but they succeed wonderfully! It almost feels like a new standard in screen acting. Think of Robert De Niro and Harvey Keitel in MEAN STEETS improvising and add one of the most subtle underpinnings of many genre clich\\xc3\\xa9s and the actors' own typecasting (Brosnan's James Bond in particular), and you got one of the most delightful pairings in recent Hollywood. <br /><br />Sadly, the story wears thin after a while. After an hour, the film just runs out of steam. Nevertheless, and I can't put my finger on it exactly, I did enjoy this very much. It just feels very fresh and original, with some imaginative use of sets and lighting, and some hints to Seijun Suzuki and Jean-Pierre Melville. The other characters aren't given much to do, but this film does offer something new, in that respect it almost effortlessly succeeds in blending all conventional genres into quite an entertaining spoof. Very amusing.<br /><br />Camera Obscura --- 7/10\"\n",
      "1\n",
      "b\"I hadn't seen this show until it was repeated on Dave. I became aware of it after seeing The Comedy Store Players, a group of comedians which includes Josie Lawrence.<br /><br />I enjoy the unpredictability of Whose Line is it Anyway and the diversity of the performers. The fact that the show had both English and American comedians made it better.<br /><br />The best performance ever had to be when Josie Lawrence and Caroline Quentin sung a duet about a beached whale. Admitedly that description may not sound funny, but if you've seen it you'll know how good it was.<br /><br />Colin Mochrie and Ryan Stiles were a great double act. They were particularly good at the round where they were given two completely random lines to incorporate into a sketch.<br /><br />Other improv shows such as Mock The Week are good, but Whose Line is it Anyway will always be the king of Improvised Comedy shows\"\n",
      "2\n",
      "b'This film, directed by New World Pictures alumnus Jonathan Kaplan and starring cult favorite Aimee Graham, is part of the \"Rebel Highway\" series of remakes of 1950s juvenile-delinquent films.<br /><br />For what was basically a chance for the filmmakers to have fun, _Reform School Girl_ is quite watchable. There are the allusions to McCarthyism characteristic of the \"Rebel Highway\" series (and a well-done general 50s ambiance), and the usual array of interesting types we meet in women-in-prison films. This one is nowhere near as graphic as a typical entry in that genre, but there is one lesbian love scene that is strikingly filmed and acted, suprisingly graphic for such young-looking actresses, and really kind of a tonal shift from the rest of the film. Unfortunately, the film never really resolves the lesbian relationship.<br /><br />The only time I really cringed was the scene where Donna (Aimee Graham) started dancing and singing with the mop, and the camera began moving around and getting in the faces of the onlookers. This was a totally ridiculous scene (but I guess it\\'s hard to pad these things out to 80 minutes).<br /><br />Graham is stellar in the title role, a girl who has had to deal with abuse and, while in reform school, awakens to more positive sexual experiences. I really wish Hollywood would take more notice of her.'\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# preview a few samples\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d16cbd",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ffb4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Having looked at our data above, we see that the raw text contains HTML break\n",
    "# tags of the form '<br />'. These tags will not be removed by the default\n",
    "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
    "# create a custom standardization function.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a308e",
   "metadata": {},
   "source": [
    "# vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a2bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1169b22",
   "metadata": {},
   "source": [
    "# Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76fc2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586fd180",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805792cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 389s 206ms/step - loss: -379634122752.0000 - accuracy: 0.1664 - val_loss: -2030338310144.0000 - val_accuracy: 0.1677\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 333s 178ms/step - loss: -14591966838784.0000 - accuracy: 0.1664 - val_loss: -36746452533248.0000 - val_accuracy: 0.1677\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 342s 182ms/step - loss: -97950462115840.0000 - accuracy: 0.1664 - val_loss: -178102781083648.0000 - val_accuracy: 0.1677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbfb68b730>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfabb5",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0896edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 63s 79ms/step - loss: 180091099283456.0000 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[180091099283456.0, 0.5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd265e1f",
   "metadata": {},
   "source": [
    "# Make an end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81648e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664/782 [========================>.....] - ETA: 6s - loss: 180110862843904.0000 - accuracy: 0.4996"
     ]
    }
   ],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0c586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
